{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import imageio\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data load & split into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Image loader\n",
    "def load_image(img_dir):\n",
    "    img_list = os.listdir(img_dir) # image file list load\n",
    "    img_data = []\n",
    "    for f_name in img_list:\n",
    "        img_data.append(imageio.imread(img_dir+f_name)/255)\n",
    "    return np.asarray(img_data)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lever_img_dir = './Door_imgs/lever/resize/'\n",
    "round_img_dir = './Door_imgs/round/resize/'\n",
    "lever_imgs = load_image(lever_img_dir)\n",
    "round_imgs = load_image(round_img_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lever images:  (2252, 256, 256, 3)\n",
      "Number of round images:  (1360, 256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "print('Number of lever images: ', lever_imgs.shape)\n",
    "print('Number of round images: ', round_imgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_set = np.append(lever_imgs,round_imgs,axis=0)\n",
    "# label_zeros = np.zeros((len(lever_imgs),1), np.float32)\n",
    "# label_ones = np.ones((len(round_imgs),1), np.float32)\n",
    "label_zeros = [[0.]]*len(lever_imgs)\n",
    "label_ones = [[1.]]*len(round_imgs)\n",
    "\n",
    "label_zeros.extend(label_ones)\n",
    "label = label_zeros \n",
    "\n",
    "## Data split into train and test set\n",
    "x_train, x_test, y_train, y_test = train_test_split(np.array(img_set,dtype=np.float32),np.array(label,dtype=np.float32),test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2889, 1)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train sets:  2889\n",
      "Number of test sets:  723\n"
     ]
    }
   ],
   "source": [
    "print('Number of train sets: ',x_train.shape[0])\n",
    "print('Number of test sets: ',x_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "learning_epoch = 100\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model build\n",
    "- Convolution -> Pooling -> Convolution -> Pooling -> FC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(shape = [None, 256, 256, 3], dtype=tf.float32, name='X')\n",
    "Y = tf.placeholder(tf.uint8, shape = [None,1], name='Y')\n",
    "Y_ohe = tf.one_hot(Y, 2)\n",
    "Y_ohe1 = tf.reshape(Y_ohe, [-1, 2])\n",
    "\n",
    "\n",
    "conv1 = tf.layers.conv2d(inputs=X, kernel_size=[5,5], strides=(1,1),filters=8, padding='same', name='conv1', activation=tf.nn.relu)\n",
    "pool1 = tf.layers.max_pooling2d(conv1,pool_size=[4,4],strides=[4,4],name='pool1')\n",
    "\n",
    "conv2 = tf.layers.conv2d(inputs=pool1, kernel_size=[5,5], strides=[1,1],filters=16, padding='SAME', name='conv2', activation=tf.nn.relu)\n",
    "pool2 = tf.layers.max_pooling2d(conv2,pool_size=[4,4],strides=[4,4],name='pool2')\n",
    "\n",
    "flat = tf.layers.flatten(pool2, name='flat')\n",
    "\n",
    "dense1=tf.layers.dense(inputs=flat, units=2000, activation=tf.nn.relu, name='dense1', kernel_initializer = tf.contrib.layers.xavier_initializer())\n",
    "dense2=tf.layers.dense(inputs=dense1, units=800, activation=tf.nn.relu, name='dense2', kernel_initializer = tf.contrib.layers.xavier_initializer())\n",
    "logit = tf.layers.dense(inputs=dense2, units=2, name='logits')\n",
    "\n",
    "\n",
    "## Estimation result\n",
    "est_res = tf.argmax(tf.nn.softmax(logit),1)\n",
    "\n",
    "## Cost function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit,labels=Y_ohe1))\n",
    "\n",
    "## Optimization of CNN model\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = lr).minimize(cost)\n",
    "\n",
    "\n",
    "correct_prediction = tf.equal(est_res,tf.argmax(Y_ohe1,1))\n",
    "\n",
    "## Calculate accuracy\n",
    "accuracy = tf.reduce_sum(tf.cast(correct_prediction,tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x_tr, y_tr):\n",
    "    return sess.run([cost, optimizer, accuracy], feed_dict = {X: x_tr, Y: y_tr})\n",
    "\n",
    "def test_est_acc(x_test, y_test):\n",
    "    return sess.run([est_res, accuracy,tf.cast(correct_prediction,tf.float32)], feed_dict = {X: x_test, Y: y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train -------------\n",
      "Epoch:  0  /  100\n",
      "Cost:  0.6621739851249443\n",
      "Accuracy:  0.6680512287988923 \n",
      "\n",
      "\n",
      "Test ---------------------\n",
      "[1 0 1 1 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1] \n",
      " [0 0 0 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0]\n",
      "Accuracy:  0.7634854771784232 \n",
      "\n",
      "\n",
      "Train -------------\n",
      "Epoch:  10  /  100\n",
      "Cost:  0.08494673145341348\n",
      "Accuracy:  0.9698857736240913 \n",
      "\n",
      "\n",
      "Test ---------------------\n",
      "[1 0 1 1 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1] \n",
      " [0 0 1 1 1 0 0 0 1 0 1 1 1 0 1 1 1 1 1]\n",
      "Accuracy:  0.8672199170124482 \n",
      "\n",
      "\n",
      "Train -------------\n",
      "Epoch:  20  /  100\n",
      "Cost:  0.005004231215914862\n",
      "Accuracy:  0.9982692973347179 \n",
      "\n",
      "\n",
      "Test ---------------------\n",
      "[1 0 1 1 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1] \n",
      " [0 0 1 0 1 0 0 0 1 0 1 1 1 0 1 0 1 1 1]\n",
      "Accuracy:  0.8976486860304288 \n",
      "\n",
      "\n",
      "Train -------------\n",
      "Epoch:  30  /  100\n",
      "Cost:  0.006992282179483451\n",
      "Accuracy:  0.9972308757355486 \n",
      "\n",
      "\n",
      "Test ---------------------\n",
      "[1 0 1 1 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1] \n",
      " [0 0 1 0 1 0 0 0 1 0 1 1 1 0 1 0 0 1 1]\n",
      "Accuracy:  0.9073305670816044 \n",
      "\n",
      "\n",
      "Train -------------\n",
      "Epoch:  40  /  100\n",
      "Cost:  0.0044727935868522665\n",
      "Accuracy:  0.9986154378677743 \n",
      "\n",
      "\n",
      "Test ---------------------\n",
      "[1 0 1 1 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1] \n",
      " [0 0 1 1 1 0 0 0 1 0 1 1 1 1 1 0 1 1 1]\n",
      "Accuracy:  0.8810511756569848 \n",
      "\n",
      "\n",
      "Train -------------\n",
      "Epoch:  50  /  100\n",
      "Cost:  0.004338965171461575\n",
      "Accuracy:  0.9989615784008308 \n",
      "\n",
      "\n",
      "Test ---------------------\n",
      "[1 0 1 1 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1] \n",
      " [0 0 1 0 1 0 0 0 1 0 1 1 1 0 0 0 0 1 1]\n",
      "Accuracy:  0.9156293222683264 \n",
      "\n",
      "\n",
      "Train -------------\n",
      "Epoch:  60  /  100\n",
      "Cost:  4.272850749811172e-06\n",
      "Accuracy:  1.0 \n",
      "\n",
      "\n",
      "Test ---------------------\n",
      "[1 0 1 1 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1] \n",
      " [0 0 1 1 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1]\n",
      "Accuracy:  0.9045643153526971 \n",
      "\n",
      "\n",
      "Train -------------\n",
      "Epoch:  70  /  100\n",
      "Cost:  2.3903252479233214e-05\n",
      "Accuracy:  1.0 \n",
      "\n",
      "\n",
      "Test ---------------------\n",
      "[1 0 1 1 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1] \n",
      " [0 0 1 1 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1]\n",
      "Accuracy:  0.9100968188105117 \n",
      "\n",
      "\n",
      "Train -------------\n",
      "Epoch:  80  /  100\n",
      "Cost:  1.5203928337169914e-06\n",
      "Accuracy:  1.0 \n",
      "\n",
      "\n",
      "Test ---------------------\n",
      "[1 0 1 1 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1] \n",
      " [0 0 1 1 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1]\n",
      "Accuracy:  0.9073305670816044 \n",
      "\n",
      "\n",
      "Train -------------\n",
      "Epoch:  90  /  100\n",
      "Cost:  4.648109556589334e-07\n",
      "Accuracy:  1.0 \n",
      "\n",
      "\n",
      "Test ---------------------\n",
      "[1 0 1 1 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1] \n",
      " [0 0 1 1 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1]\n",
      "Accuracy:  0.9073305670816044 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Tensorflow session \n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "##\n",
    "tr_batch_len = x_train.shape[0]//batch_size+1\n",
    "\n",
    "for epoch in range(learning_epoch):\n",
    "    data_index = list(np.arange(0,x_train.shape[0]))\n",
    "    random.shuffle(data_index)\n",
    "    co = 0\n",
    "    acc = 0\n",
    "\n",
    "    ## Training\n",
    "    for batch in range(tr_batch_len):\n",
    "        ## Train model by mini-batch dataset\n",
    "        y_tr_input = y_train[data_index[batch*batch_size:(batch+1)*batch_size]]\n",
    "        x_tr_input = x_train[data_index[batch*batch_size:(batch+1)*batch_size],:,:,:]\n",
    "        c, _, a = train(x_tr_input, y_tr_input)\n",
    "        \n",
    "        ## Accuracy \n",
    "        co+=c\n",
    "        acc += a\n",
    "\n",
    "    co /= tr_batch_len\n",
    "    acc /= x_train.shape[0]\n",
    "    \n",
    "    if epoch%10==0:\n",
    "        print('Train -------------')\n",
    "        print('Epoch: ',epoch,' / ',learning_epoch)\n",
    "        print('Cost: ',co)\n",
    "        print('Accuracy: ',acc,'\\n\\n')\n",
    "    \n",
    "        ## Test\n",
    "        test_batch_len = x_test.shape[0]//batch_size+1\n",
    "        acc_test = 0\n",
    "        for batch_est in range(test_batch_len):\n",
    "            x_test_input = x_test[batch_est*batch_size:(batch_est+1)*batch_size]\n",
    "            y_test_input = y_test[batch_est*batch_size:(batch_est+1)*batch_size]\n",
    "            er_test, ac_test,temp = test_est_acc(x_test_input,y_test_input)\n",
    "            acc_test += ac_test\n",
    "            \n",
    "        acc_test /= x_test.shape[0]\n",
    "        print('Test ---------------------')\n",
    "        print(np.asarray(np.reshape(y_test_input,-1),dtype=np.int16),'\\n',er_test)\n",
    "        print('Accuracy: ',acc_test,'\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
